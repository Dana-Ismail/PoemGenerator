{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "UUg1xAox7w7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# English Poem"
      ],
      "metadata": {
        "id": "EzhW6CuF_0ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import string\n",
        "import itertools\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import torch.optim as optim\n",
        "from google.colab import drive\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords\n",
        "from tokenizers.models import BPE\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GPT2Tokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import Sequence, NFKC\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from tokenizers import Tokenizer, normalizers, pre_tokenizers, processors\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "CXphEUZuAS5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate and authorize Google Drive access\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "IENmZddE_3If",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc14cc4-7323-4070-af3d-121b7577c687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "aPhiA4qVC8AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/PoetryFoundationData.csv'\n",
        "dataset = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "ipdQfHeBLlL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_statistics(data):\n",
        "    num_samples = len(data)\n",
        "    num_characters = sum(len(text) for text in data['Poem'])\n",
        "    num_words = sum(len(text.split()) for text in data['Poem'])\n",
        "    unique_words = set(word for text in data['Poem'] for word in text.split())\n",
        "    num_unique_words = len(unique_words)\n",
        "    avg_words_per_sample = num_words / num_samples\n",
        "\n",
        "    statistics = {\n",
        "        'Number of Samples': num_samples,\n",
        "        'Number of Characters': num_characters,\n",
        "        'Number of Words': num_words,\n",
        "        'Number of Unique Words': num_unique_words,\n",
        "        'Average Words per Sample': avg_words_per_sample\n",
        "    }\n",
        "\n",
        "    return statistics"
      ],
      "metadata": {
        "id": "iUVE80CVCHhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_data_statistics(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0deBhRwjCR1L",
        "outputId": "7a21535d-fb36-441a-c9e8-5970c44892f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Number of Samples': 13854,\n",
              " 'Number of Characters': 20856813,\n",
              " 'Number of Words': 3461519,\n",
              " 'Number of Unique Words': 274687,\n",
              " 'Average Words per Sample': 249.85700880612097}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select relevant columns (title and poem)\n",
        "data = dataset[['Title', 'Poem']].head(900)\n",
        "\n",
        "# preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # lowercase the text\n",
        "    text = text.lower()\n",
        "    # remove punctuation and special characters\n",
        "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # join the tokens back into a single string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "hbtaszBmRH9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a5a82a-ce1f-4557-d6d3-54d4f836a84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_data_statistics(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCg7kJWVCqoM",
        "outputId": "a6752e8c-a67c-46e1-edf5-d3a56af579b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Number of Samples': 900,\n",
              " 'Number of Characters': 1542475,\n",
              " 'Number of Words': 256615,\n",
              " 'Number of Unique Words': 45321,\n",
              " 'Average Words per Sample': 285.1277777777778}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessing to the poem column\n",
        "data['Poem'] = data['Poem'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "X9eKbRhzYXIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Poem'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gZ3HaqEwVBMN",
        "outputId": "c13ba534-160f-41fd-b116-bfb63ab6f6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog bone stapler cribbage board garlic press window looselacks suction lack grip bungee cord bootstrap dog leash leather belt window sash cord frayed broke feather duster thatch straw empty bottle elmers glue window loudits hinge clack open clack shut stuffed bear baby blanket single crib newel window split dividing two velvet moss sagebrush willow branch robin wing window paneless frame air'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Poem'])\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# convert the preprocessed poems to sequences of indices\n",
        "encoded_poems = tokenizer.texts_to_sequences(data['Poem'])"
      ],
      "metadata": {
        "id": "_3n2FXH6eZqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad the encoded sequences for equal length\n",
        "max_sequence_length = max(len(seq) for seq in encoded_poems)\n",
        "padded_poems = pad_sequences(encoded_poems, maxlen=max_sequence_length)"
      ],
      "metadata": {
        "id": "vPwClHouUj8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 3 # window size // the model will consider a context of 3 words to predict the next word\n",
        "\n",
        "# initialize lists to store input sequences (X) and target labels (y)\n",
        "input_sequences = []\n",
        "target_labels = []\n",
        "\n",
        "# iterate over each encoded poem\n",
        "for poem in encoded_poems:\n",
        "    # slide the window over the poem\n",
        "    for i in range(len(poem) - window_size):\n",
        "        # extract the input sequence and target label for the current window\n",
        "        window = poem[i:i+window_size]\n",
        "        target = poem[i+window_size]\n",
        "\n",
        "        # append the input sequence and target label to the lists\n",
        "        input_sequences.append(window)\n",
        "        target_labels.append(target)"
      ],
      "metadata": {
        "id": "O5UloKnNmk5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "S9BRYvsBZCTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation sets\n",
        "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_sequences, target_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# convert sequences to numpy arrays\n",
        "train_inputs = np.array(train_inputs)\n",
        "train_targets = np.array(train_targets)\n",
        "val_inputs = np.array(val_inputs)\n",
        "val_targets = np.array(val_targets)\n",
        "\n",
        "# normalize input sequences\n",
        "train_inputs = train_inputs / vocabulary_size\n",
        "val_inputs = val_inputs / vocabulary_size\n",
        "\n",
        "# convert target sequences to one-hot encoded vectors\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
        "val_targets = to_categorical(val_targets, num_classes=vocabulary_size)"
      ],
      "metadata": {
        "id": "1k4uaZ1UUv3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, 100, input_length=window_size))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))"
      ],
      "metadata": {
        "id": "vhphgQTZfPXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "c0YUYLqKS4Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_inputs, train_targets, validation_data=(val_inputs, val_targets), batch_size=32, epochs=10)"
      ],
      "metadata": {
        "id": "NCUhFCQRnH95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5107ccaf-c1cb-408f-94a9-719909e7ff02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch\n",
            "1/10 3401/3401 [==============================] - 137s 40ms/step - loss: 9.0251 - val_loss: 8.9570\n",
            "Epoch\n",
            "2/103401/3401 [==============================] - 147s 43ms/step - loss: 8.6695 - val_loss: 9.1011\n",
            "Epoch\n",
            "3/103401/3401 [==============================] - 128s 38ms/step - loss: 8.6323 - val_loss: 9.2342\n",
            "Epoch\n",
            "4/103401/3401 [==============================] - 132s 39ms/step - loss: 8.6211 - val_loss: 9.3207\n",
            "Epoch\n",
            "5/103401/3401 [==============================] - 131s 38ms/step - loss: 8.6155 - val_loss: 9.3775\n",
            "Epoch\n",
            "6/103401/3401 [==============================] - 129s 38ms/step - loss: 8.6119 - val_loss: 9.4389\n",
            "Epoch\n",
            "7/103401/3401 [==============================] - 130s 38ms/step - loss: 8.6090 - val_loss: 9.4358\n",
            "Epoch\n",
            "8/103401/3401 [==============================] - 129s 38ms/step - loss: 8.6067 - val_loss: 9.4309\n",
            "Epoch\n",
            "9/103401/3401 [==============================] - 128s 38ms/step - loss: 8.6051 - val_loss: 9.4777\n",
            "Epoch\n",
            "10/103401/3401 [==============================] - 131s 38ms/step - loss: 8.6035 - val_loss: 9.4876\n",
            "<keras.callbacks.History at 0x1eb6b5eecd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, input_sequences, target_labels):\n",
        "    # convert input sequences and target labels to numpy arrays\n",
        "    input_sequences = np.array(input_sequences)\n",
        "    target_labels = np.array(target_labels)\n",
        "    # generate predictions using the model\n",
        "    predictions = model.predict(input_sequences)\n",
        "    # calculate cross-entropy loss\n",
        "    cross_entropy = -np.log(predictions[np.arange(len(target_labels)), target_labels])\n",
        "    # calculate average loss\n",
        "    average_loss = np.mean(cross_entropy)\n",
        "    # calculate perplexity\n",
        "    perplexity = np.exp(average_loss)\n",
        "\n",
        "    return perplexity, average_loss\n",
        "\n",
        "perplexity, avg_loss = calculate_perplexity(model, input_sequences, target_labels)\n",
        "print(\"Perplexity:\", perplexity)\n",
        "print(\"Avg_LOss:\", avg_loss)"
      ],
      "metadata": {
        "id": "PJyiz0up4z30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9527c78-944e-42c9-a545-736352e27486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4251/4251 [==============================] - 36s 8ms/step\n",
            " Perplexity: 6324.4854\n",
            " Avg_LOss: 8.752184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poem(model, tokenizer, seed_text, num_words):\n",
        "    generated_poem = seed_text\n",
        "    for _ in range(num_words):\n",
        "        # tokenize the seed text\n",
        "        tokenized_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        # pad the tokenized text for input to the model\n",
        "        padded_text = pad_sequences([tokenized_text], maxlen=max_sequence_length)\n",
        "        # generate the next word probabilities using the model\n",
        "        predicted_probs = model.predict(padded_text, verbose=0)[0]\n",
        "        # get the index of the predicted word with the highest probability\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "        # convert the predicted word index to the actual word\n",
        "        predicted_word = tokenizer.index_word[predicted_index]\n",
        "        # add the predicted word to the generated poem\n",
        "        generated_poem += \" \" + predicted_word\n",
        "        # update the seed text with the predicted word\n",
        "        seed_text += \" \" + predicted_word\n",
        "\n",
        "    return generated_poem\n",
        "\n",
        "# user topic or seed text\n",
        "user_topic = \"sun\"\n",
        "num_words = 20  # poem length\n",
        "\n",
        "generated_poem = generate_poem(model, tokenizer, user_topic, num_words)\n",
        "print(\"Generated Poem:\")\n",
        "print(generated_poem)"
      ],
      "metadata": {
        "id": "N6tyaNhg6qM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a701f3-c16a-42d9-8b15-25c20523cb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Poem:sun like like like like like like like like like like like like like like like like like like like like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QO6MGiXK1yF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}